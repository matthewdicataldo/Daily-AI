const std = @import("std");
const claude = @import("ai_claude.zig");
const http = @import("common_http.zig");
const firecrawl = @import("external_firecrawl.zig");
const types = @import("core_types.zig");
const llm_first_search = @import("ai_llm_first_search.zig");

/// Research complexity levels
pub const ResearchComplexity = enum {
    simple, // Single-hop, direct search
    moderate, // Multi-hop, 2-3 search iterations
    complex, // Deep multi-hop with replanning
    comprehensive, // Full exploration with multiple strategies

    pub fn getMaxIterations(self: ResearchComplexity) u32 {
        return switch (self) {
            .simple => 3,
            .moderate => 8,
            .complex => 15,
            .comprehensive => 25,
        };
    }

    pub fn getMaxReplans(self: ResearchComplexity) u32 {
        return switch (self) {
            .simple => 0,
            .moderate => 1,
            .complex => 2,
            .comprehensive => 3,
        };
    }
};

/// Research plan generated by LLM
pub const ResearchPlan = struct {
    original_query: []const u8,
    decomposed_queries: [][]const u8,
    search_strategy: []const u8,
    expected_sources: [][]const u8,
    complexity: ResearchComplexity,
    confidence: f32,

    pub fn deinit(self: ResearchPlan, allocator: std.mem.Allocator) void {
        allocator.free(self.original_query);
        for (self.decomposed_queries) |query| {
            allocator.free(query);
        }
        allocator.free(self.decomposed_queries);
        allocator.free(self.search_strategy);
        for (self.expected_sources) |source| {
            allocator.free(source);
        }
        allocator.free(self.expected_sources);
    }
};

/// Research state for tracking progress
pub const ResearchState = struct {
    plan: ResearchPlan,
    current_iteration: u32,
    replan_count: u32,
    completed_queries: std.ArrayList([]const u8),
    gathered_evidence: std.ArrayList(types.NewsItem),
    synthesis_notes: std.ArrayList([]const u8),
    confidence_scores: std.ArrayList(f32),

    allocator: std.mem.Allocator,

    pub fn init(allocator: std.mem.Allocator, plan: ResearchPlan) ResearchState {
        return ResearchState{
            .plan = plan,
            .current_iteration = 0,
            .replan_count = 0,
            .completed_queries = std.ArrayList([]const u8).init(allocator),
            .gathered_evidence = std.ArrayList(types.NewsItem).init(allocator),
            .synthesis_notes = std.ArrayList([]const u8).init(allocator),
            .confidence_scores = std.ArrayList(f32).init(allocator),
            .allocator = allocator,
        };
    }

    pub fn deinit(self: *ResearchState) void {
        self.plan.deinit(self.allocator);

        for (self.completed_queries.items) |query| {
            self.allocator.free(query);
        }
        self.completed_queries.deinit();

        for (self.gathered_evidence.items) |evidence| {
            evidence.deinit(self.allocator);
        }
        self.gathered_evidence.deinit();

        for (self.synthesis_notes.items) |note| {
            self.allocator.free(note);
        }
        self.synthesis_notes.deinit();

        self.confidence_scores.deinit();
    }
};

/// Research result with comprehensive synthesis
pub const ResearchResult = struct {
    original_query: []const u8,
    comprehensive_answer: []const u8,
    key_findings: [][]const u8,
    evidence_sources: []types.NewsItem,
    confidence_score: f32,
    research_depth: u32,
    total_sources_examined: u32,

    pub fn deinit(self: ResearchResult, allocator: std.mem.Allocator) void {
        allocator.free(self.original_query);
        allocator.free(self.comprehensive_answer);
        for (self.key_findings) |finding| {
            allocator.free(finding);
        }
        allocator.free(self.key_findings);
        for (self.evidence_sources) |source| {
            source.deinit(allocator);
        }
        allocator.free(self.evidence_sources);
    }
};

/// Deep Research System inspired by OpenDeepSearch and OpenProbe
pub const DeepResearchSystem = struct {
    allocator: std.mem.Allocator,
    claude_client: *claude.ClaudeClient,
    http_client: *http.HttpClient,
    firecrawl_client: *firecrawl.FirecrawlClient,

    // Research configuration
    default_complexity: ResearchComplexity,
    enable_caching: bool,
    enable_reranking: bool,
    max_concurrent_searches: u32,

    // LLM-First Search configuration (defaults to simple mode)
    llm_search_engine: ?llm_first_search.LLMFirstSearchEngine,

    // Research metrics
    total_research_sessions: u32,
    successful_syntheses: u32,
    cache_hits: u32,
    average_research_depth: f32,

    const Self = @This();

    pub fn init(allocator: std.mem.Allocator, claude_client: *claude.ClaudeClient, http_client: *http.HttpClient, firecrawl_client: *firecrawl.FirecrawlClient) Self {
        return Self{
            .allocator = allocator,
            .claude_client = claude_client,
            .http_client = http_client,
            .firecrawl_client = firecrawl_client,
            .default_complexity = .moderate,
            .enable_caching = true,
            .enable_reranking = true,
            .max_concurrent_searches = 3,
            .llm_search_engine = null,
            .total_research_sessions = 0,
            .successful_syntheses = 0,
            .cache_hits = 0,
            .average_research_depth = 0.0,
        };
    }

    pub fn deinit(self: *Self) void {
        if (self.llm_search_engine) |*search_engine| {
            search_engine.deinit();
        }
    }

    /// Initialize research components with LLM-First Search
    pub fn initialize(self: *Self) void {
        // Initialize LLM-First Search Engine
        self.llm_search_engine = llm_first_search.createLLMFirstSearchEngine(self.allocator, self.claude_client);
        self.llm_search_engine.?.enableSimpleMode(); // Use paper-inspired simple mode
        
        std.log.info("ðŸ”¬ Deep Research System: Ready for LLM-guided research operations", .{});
    }

    /// Execute comprehensive deep research
    pub fn executeDeepResearch(self: *Self, query: []const u8, complexity: ?ResearchComplexity) !ResearchResult {
        const research_complexity = complexity orelse self.default_complexity;

        std.log.info("ðŸ”¬ Starting deep research: {s} (complexity: {s})", .{ query, @tagName(research_complexity) });
        self.total_research_sessions += 1;

        // Check cache first
        if (self.enable_caching) {
            if (try self.checkSimpleCache(query)) |cached_content| {
                defer self.allocator.free(cached_content);
                self.cache_hits += 1;
                std.log.info("ðŸ“‹ Found cached research result");
                // Return a simple cached result
                return ResearchResult{
                    .original_query = try self.allocator.dupe(u8, query),
                    .comprehensive_answer = try self.allocator.dupe(u8, cached_content),
                    .key_findings = try self.allocator.dupe([]const u8, &[_][]const u8{
                        try self.allocator.dupe(u8, "Cached research result"),
                    }),
                    .evidence_sources = &[_]types.NewsItem{},
                    .confidence_score = 0.8,
                    .research_depth = 1,
                    .total_sources_examined = 0,
                };
            }
        }

        // Generate research plan using LLM
        const research_plan = try self.generateResearchPlan(query, research_complexity);
        var research_state = ResearchState.init(self.allocator, research_plan);
        defer research_state.deinit();

        const max_iterations = research_complexity.getMaxIterations();
        const max_replans = research_complexity.getMaxReplans();

        // Execute research iterations
        while (research_state.current_iteration < max_iterations) {
            defer research_state.current_iteration += 1;

            std.log.info("ðŸ”„ Research iteration {d}/{d}", .{ research_state.current_iteration + 1, max_iterations });

            // Execute current research phase
            const iteration_successful = try self.executeResearchIteration(&research_state);

            // Evaluate progress and decide if replanning is needed
            const should_replan = try self.shouldReplan(&research_state);

            if (should_replan and research_state.replan_count < max_replans) {
                std.log.info("ðŸ”„ Replanning research strategy...");
                try self.replanResearch(&research_state);
                research_state.replan_count += 1;
            }

            // Check if we have sufficient information for synthesis
            const ready_for_synthesis = try self.isReadyForSynthesis(&research_state);
            if (ready_for_synthesis and iteration_successful) {
                break;
            }
        }

        // Synthesize final research result
        const result = try self.synthesizeResearchResult(&research_state);

        // Cache the result
        if (self.enable_caching) {
            try self.writeSimpleCache(query, result.comprehensive_answer);
        }

        self.successful_syntheses += 1;
        self.average_research_depth = (self.average_research_depth * @as(f32, @floatFromInt(self.total_research_sessions - 1)) +
            @as(f32, @floatFromInt(research_state.current_iteration))) /
            @as(f32, @floatFromInt(self.total_research_sessions));

        std.log.info("âœ… Deep research completed: {d} iterations, {d} sources, confidence: {d:.2}", .{ research_state.current_iteration, research_state.gathered_evidence.items.len, result.confidence_score });

        return result;
    }

    /// Generate comprehensive research plan using LLM
    fn generateResearchPlan(self: *Self, query: []const u8, complexity: ResearchComplexity) !ResearchPlan {
        const planning_prompt = try std.fmt.allocPrint(self.allocator,
            \\You are a research strategist. Create a comprehensive research plan for this query.
            \\
            \\Query: {s}
            \\Complexity Level: {s}
            \\
            \\Decompose this into specific, actionable research queries. Consider:
            \\1. Key concepts that need investigation
            \\2. Related topics and context
            \\3. Different perspectives and sources
            \\4. Verification and fact-checking needs
            \\
            \\Respond in JSON format:
            \\{{
            \\  "decomposed_queries": ["specific query 1", "specific query 2", ...],
            \\  "search_strategy": "description of approach",
            \\  "expected_sources": ["source type 1", "source type 2", ...],
            \\  "confidence": 0.0-1.0
            \\}}
        , .{ query, @tagName(complexity) });
        defer self.allocator.free(planning_prompt);

        const llm_response = try self.claude_client.executeClaude(planning_prompt);
        defer self.allocator.free(llm_response);

        return try self.parseResearchPlan(llm_response, query, complexity);
    }

    /// Execute a single research iteration
    fn executeResearchIteration(self: *Self, state: *ResearchState) !bool {
        // Determine which queries to execute this iteration
        const queries_to_execute = try self.selectQueriesForIteration(state);
        defer {
            for (queries_to_execute) |query| {
                self.allocator.free(query);
            }
            self.allocator.free(queries_to_execute);
        }

        if (queries_to_execute.len == 0) {
            return false;
        }

        // Execute searches using LLM-First Search or fallback to simple web search
        var iteration_results = std.ArrayList(types.NewsItem).init(self.allocator);
        defer iteration_results.deinit();

        if (self.llm_search_engine) |*search_engine| {
            // Use LLM-First Search for intelligent query execution
            for (queries_to_execute) |query| {
                std.log.info("ðŸ¤– Executing LLM-guided search: {s}", .{query});
                
                // Create simple search interface for LLM-First Search
                const search_interface = SimpleSearchInterface{
                    .deep_research_system = self,
                };
                
                const llm_search_results = try search_engine.search(query, search_interface);
                defer {
                    for (llm_search_results) |result| {
                        result.deinit(self.allocator);
                    }
                    self.allocator.free(llm_search_results);
                }

                // Add LLM-guided results to iteration results
                for (llm_search_results) |result| {
                    try iteration_results.append(try result.clone(self.allocator));
                }

                // Mark query as completed
                try state.completed_queries.append(try self.allocator.dupe(u8, query));
            }
        } else {
            // Fallback to simple web search
            for (queries_to_execute) |query| {
                std.log.info("ðŸ” Executing simple search: {s}", .{query});

                // Use Firecrawl for web search
                const search_results = try self.performWebSearch(query);
                defer {
                    for (search_results) |result| {
                        result.deinit(self.allocator);
                    }
                    self.allocator.free(search_results);
                }

                // Add to iteration results
                for (search_results) |result| {
                    try iteration_results.append(try result.clone(self.allocator));
                }

                // Mark query as completed
                try state.completed_queries.append(try self.allocator.dupe(u8, query));
            }
        }

        // Apply semantic reranking if enabled
        var ranked_results = iteration_results.items;
        if (self.enable_reranking) {
            ranked_results = try self.applySemanticReranking(iteration_results.items, state.plan.original_query);
        }

        // Add results to evidence
        for (ranked_results) |result| {
            try state.gathered_evidence.append(try result.clone(self.allocator));
        }

        // Generate synthesis notes for this iteration
        const synthesis_note = try self.generateIterationSynthesis(state, ranked_results);
        try state.synthesis_notes.append(synthesis_note);

        // Calculate confidence for this iteration
        const iteration_confidence = try self.calculateIterationConfidence(ranked_results);
        try state.confidence_scores.append(iteration_confidence);

        return ranked_results.len > 0;
    }

    /// Check if replanning is needed based on current progress
    fn shouldReplan(self: *Self, state: *ResearchState) !bool {
        if (state.gathered_evidence.items.len == 0) {
            return true; // No results, definitely need to replan
        }

        // Calculate average confidence
        var total_confidence: f32 = 0.0;
        for (state.confidence_scores.items) |confidence| {
            total_confidence += confidence;
        }
        const avg_confidence = total_confidence / @as(f32, @floatFromInt(state.confidence_scores.items.len));

        // Use LLM to evaluate if replanning is needed
        const evaluation_prompt = try std.fmt.allocPrint(self.allocator,
            \\Evaluate if the current research progress needs replanning.
            \\
            \\Original Query: {s}
            \\Completed Queries: {d}
            \\Evidence Gathered: {d} sources
            \\Average Confidence: {d:.2}
            \\Current Iteration: {d}
            \\
            \\Should we replan the research strategy? Consider:
            \\1. Are we getting relevant results?
            \\2. Are there gaps in the research?
            \\3. Is the current approach effective?
            \\
            \\Respond with: REPLAN or CONTINUE
        , .{ state.plan.original_query, state.completed_queries.items.len, state.gathered_evidence.items.len, avg_confidence, state.current_iteration });
        defer self.allocator.free(evaluation_prompt);

        const llm_response = try self.claude_client.executeClaude(evaluation_prompt);
        defer self.allocator.free(llm_response);

        return std.mem.indexOf(u8, llm_response, "REPLAN") != null;
    }

    /// Replan research strategy based on current progress
    fn replanResearch(self: *Self, state: *ResearchState) !void {
        // Generate evidence summary for replanning
        var evidence_summary = std.ArrayList(u8).init(self.allocator);
        defer evidence_summary.deinit();

        const writer = evidence_summary.writer();
        try writer.print("Current Evidence Summary:\n");

        for (state.gathered_evidence.items[0..@min(5, state.gathered_evidence.items.len)]) |evidence| {
            try writer.print("- {s} (score: {d:.2})\n", .{ evidence.title, evidence.relevance_score });
        }

        const replan_prompt = try std.fmt.allocPrint(self.allocator,
            \\Replan the research strategy based on current progress.
            \\
            \\Original Query: {s}
            \\{s}
            \\
            \\Generate new research queries to fill gaps and improve results.
            \\
            \\Respond in JSON format:
            \\{{
            \\  "new_queries": ["query1", "query2", ...],
            \\  "strategy_adjustment": "description of changes"
            \\}}
        , .{ state.plan.original_query, evidence_summary.items });
        defer self.allocator.free(replan_prompt);

        const llm_response = try self.claude_client.executeClaude(replan_prompt);
        defer self.allocator.free(llm_response);

        // Parse and update research plan
        try self.updateResearchPlan(state, llm_response);

        std.log.info("ðŸ”„ Research plan updated with new strategy");
    }

    /// Check if ready for final synthesis
    fn isReadyForSynthesis(self: *Self, state: *ResearchState) !bool {
        if (state.gathered_evidence.items.len < 3) {
            return false; // Need minimum evidence
        }

        // Use LLM to evaluate synthesis readiness
        const readiness_prompt = try std.fmt.allocPrint(self.allocator,
            \\Evaluate if we have sufficient information to synthesize a comprehensive answer.
            \\
            \\Query: {s}
            \\Evidence Sources: {d}
            \\Research Iterations: {d}
            \\
            \\Do we have enough diverse, high-quality information to provide a complete answer?
            \\
            \\Respond with: READY or CONTINUE
        , .{ state.plan.original_query, state.gathered_evidence.items.len, state.current_iteration });
        defer self.allocator.free(readiness_prompt);

        const llm_response = try self.claude_client.executeClaude(readiness_prompt);
        defer self.allocator.free(llm_response);

        return std.mem.indexOf(u8, llm_response, "READY") != null;
    }

    /// Synthesize final comprehensive research result
    fn synthesizeResearchResult(self: *Self, state: *ResearchState) !ResearchResult {
        // Build comprehensive evidence summary
        var evidence_text = std.ArrayList(u8).init(self.allocator);
        defer evidence_text.deinit();

        const writer = evidence_text.writer();
        try writer.print("Research Evidence for: {s}\n\n", .{state.plan.original_query});

        for (state.gathered_evidence.items, 0..) |evidence, i| {
            try writer.print("{d}. {s}\n", .{ i + 1, evidence.title });
            try writer.print("   Source: {s}\n", .{evidence.source});
            try writer.print("   Summary: {s}\n", .{evidence.summary});
            try writer.print("   Relevance: {d:.2}\n\n", .{evidence.relevance_score});
        }

        // Generate comprehensive synthesis
        const synthesis_prompt = try std.fmt.allocPrint(self.allocator,
            \\Synthesize a comprehensive answer based on the research evidence.
            \\
            \\Query: {s}
            \\
            \\{s}
            \\
            \\Provide:
            \\1. A comprehensive answer (2-3 paragraphs)
            \\2. Key findings (3-5 bullet points)
            \\3. Confidence assessment (0.0-1.0)
            \\
            \\Format as JSON:
            \\{{
            \\  "comprehensive_answer": "detailed answer here",
            \\  "key_findings": ["finding 1", "finding 2", ...],
            \\  "confidence": 0.0-1.0
            \\}}
        , .{ state.plan.original_query, evidence_text.items });
        defer self.allocator.free(synthesis_prompt);

        const llm_response = try self.claude_client.executeClaude(synthesis_prompt);
        defer self.allocator.free(llm_response);

        return try self.parseResearchResult(llm_response, state);
    }

    /// Perform web search using Firecrawl
    fn performWebSearch(self: *Self, query: []const u8) ![]types.NewsItem {
        var all_results = std.ArrayList(types.NewsItem).init(self.allocator);
        defer all_results.deinit();

        // Construct search URL (using DuckDuckGo as example)
        const search_url = try std.fmt.allocPrint(self.allocator, "https://duckduckgo.com/?q={s}", .{query});
        defer self.allocator.free(search_url);

        // Use Firecrawl to scrape search results
        const scraped_content = self.firecrawl_client.scrapeUrl(search_url) catch |err| {
            std.log.warn("Web search failed: {}", .{err});
            return &[_]types.NewsItem{};
        };
        defer self.allocator.free(scraped_content.markdown);

        // Convert scraped content to NewsItem
        const news_item = types.NewsItem{
            .title = try std.fmt.allocPrint(self.allocator, "Search Results: {s}", .{query}),
            .summary = try self.allocator.dupe(u8, scraped_content.markdown[0..@min(200, scraped_content.markdown.len)]),
            .url = try self.allocator.dupe(u8, search_url),
            .source = try self.allocator.dupe(u8, "Web Search"),
            .timestamp = types.getCurrentTimestamp(),
            .source_type = .web_crawl,
            .relevance_score = 0.7,
            .reddit_metadata = null,
            .youtube_metadata = null,
            .huggingface_metadata = null,
            .blog_metadata = null,
            .github_metadata = null,
        };

        try all_results.append(news_item);
        return try all_results.toOwnedSlice();
    }

    // Simple cache methods using file system
    fn checkSimpleCache(self: *Self, query: []const u8) !?[]const u8 {
        const cache_key = try self.generateCacheKey(query);
        defer self.allocator.free(cache_key);
        
        const cache_path = try std.fmt.allocPrint(self.allocator, ".cache/research_{s}.txt", .{cache_key});
        defer self.allocator.free(cache_path);
        
        const cached_data = std.fs.cwd().readFileAlloc(self.allocator, cache_path, 1024 * 1024) catch |err| {
            if (err == error.FileNotFound) return null;
            std.log.warn("Cache read failed: {}", .{err});
            return null;
        };
        
        return cached_data;
    }
    
    fn writeSimpleCache(self: *Self, query: []const u8, content: []const u8) !void {
        const cache_key = try self.generateCacheKey(query);
        defer self.allocator.free(cache_key);
        
        std.fs.cwd().makeDir(".cache") catch |err| {
            if (err != error.PathAlreadyExists) {
                std.log.warn("Failed to create cache dir: {}", .{err});
                return;
            }
        };
        
        const cache_path = try std.fmt.allocPrint(self.allocator, ".cache/research_{s}.txt", .{cache_key});
        defer self.allocator.free(cache_path);
        
        std.fs.cwd().writeFile(cache_path, content) catch |err| {
            std.log.warn("Cache write failed: {}", .{err});
        };
    }
    
    fn generateCacheKey(self: *Self, query: []const u8) ![]const u8 {
        var hasher = std.hash.Wyhash.init(42);
        hasher.update(query);
        const hash = hasher.final();
        return try std.fmt.allocPrint(self.allocator, "{x}", .{hash});
    }

    fn parseResearchPlan(self: *Self, llm_response: []const u8, query: []const u8, complexity: ResearchComplexity) !ResearchPlan {
        // Try to parse JSON from LLM response
        const json_start = std.mem.indexOf(u8, llm_response, "{") orelse {
            return self.createFallbackPlan(query, complexity);
        };
        const json_end = std.mem.lastIndexOf(u8, llm_response, "}") orelse {
            return self.createFallbackPlan(query, complexity);
        };

        if (json_end <= json_start) {
            return self.createFallbackPlan(query, complexity);
        }

        const json_slice = llm_response[json_start .. json_end + 1];

        var arena = std.heap.ArenaAllocator.init(self.allocator);
        defer arena.deinit();
        const temp_allocator = arena.allocator();

        const parsed = std.json.parseFromSlice(std.json.Value, temp_allocator, json_slice, .{}) catch {
            return self.createFallbackPlan(query, complexity);
        };
        defer parsed.deinit();

        const json_obj = parsed.value.object;

        // Parse decomposed queries
        var queries = std.ArrayList([]const u8).init(self.allocator);
        if (json_obj.get("decomposed_queries")) |queries_val| {
            if (queries_val == .array) {
                for (queries_val.array.items) |query_val| {
                    if (query_val == .string) {
                        const query_str = try self.allocator.dupe(u8, query_val.string);
                        try queries.append(query_str);
                    }
                }
            }
        }

        // Fallback to basic queries if none parsed
        if (queries.items.len == 0) {
            try queries.append(try std.fmt.allocPrint(self.allocator, "{s} overview", .{query}));
            try queries.append(try std.fmt.allocPrint(self.allocator, "{s} recent news", .{query}));
            try queries.append(try std.fmt.allocPrint(self.allocator, "{s} details", .{query}));
        }

        // Parse strategy
        const strategy = if (json_obj.get("search_strategy")) |strategy_val|
            if (strategy_val == .string) try self.allocator.dupe(u8, strategy_val.string) else try self.allocator.dupe(u8, "Multi-hop semantic search")
        else
            try self.allocator.dupe(u8, "Multi-hop semantic search");

        // Parse expected sources
        var sources = std.ArrayList([]const u8).init(self.allocator);
        if (json_obj.get("expected_sources")) |sources_val| {
            if (sources_val == .array) {
                for (sources_val.array.items) |source_val| {
                    if (source_val == .string) {
                        const source_str = try self.allocator.dupe(u8, source_val.string);
                        try sources.append(source_str);
                    }
                }
            }
        }

        // Fallback sources
        if (sources.items.len == 0) {
            try sources.append(try self.allocator.dupe(u8, "web"));
            try sources.append(try self.allocator.dupe(u8, "news"));
            try sources.append(try self.allocator.dupe(u8, "academic"));
        }

        // Parse confidence
        const confidence = if (json_obj.get("confidence")) |conf_val| blk: {
            switch (conf_val) {
                .float => break :blk @as(f32, @floatCast(conf_val.float)),
                .integer => break :blk @as(f32, @floatFromInt(conf_val.integer)),
                else => break :blk 0.8,
            }
        } else 0.8;

        return ResearchPlan{
            .original_query = try self.allocator.dupe(u8, query),
            .decomposed_queries = try queries.toOwnedSlice(),
            .search_strategy = strategy,
            .expected_sources = try sources.toOwnedSlice(),
            .complexity = complexity,
            .confidence = confidence,
        };
    }

    fn createFallbackPlan(self: *Self, query: []const u8, complexity: ResearchComplexity) !ResearchPlan {
        return ResearchPlan{
            .original_query = try self.allocator.dupe(u8, query),
            .decomposed_queries = try self.allocator.dupe([]const u8, &[_][]const u8{
                try std.fmt.allocPrint(self.allocator, "{s} overview", .{query}),
                try std.fmt.allocPrint(self.allocator, "{s} recent developments", .{query}),
                try std.fmt.allocPrint(self.allocator, "{s} details", .{query}),
            }),
            .search_strategy = try self.allocator.dupe(u8, "Multi-hop semantic search"),
            .expected_sources = try self.allocator.dupe([]const u8, &[_][]const u8{
                try self.allocator.dupe(u8, "web"),
                try self.allocator.dupe(u8, "news"),
                try self.allocator.dupe(u8, "academic"),
            }),
            .complexity = complexity,
            .confidence = 0.7,
        };
    }

    fn selectQueriesForIteration(self: *Self, state: *ResearchState) ![][]const u8 {
        // Select next queries based on current iteration and progress
        const remaining_queries = state.plan.decomposed_queries.len - state.completed_queries.items.len;
        if (remaining_queries == 0) {
            return &[_][]const u8{};
        }

        const queries_this_iteration = @min(self.max_concurrent_searches, remaining_queries);
        var selected = try self.allocator.alloc([]const u8, queries_this_iteration);

        const start_idx = state.completed_queries.items.len;
        for (0..queries_this_iteration) |i| {
            // Safe array access with bounds checking to prevent memory corruption
            const target_idx = start_idx + i;
            if (target_idx >= state.plan.decomposed_queries.len) {
                std.log.err("âŒ CRITICAL: Invalid array access in selectQueriesForIteration - index {d} >= length {d}", .{target_idx, state.plan.decomposed_queries.len});
                return error.IndexOutOfBounds;
            }
            selected[i] = try self.allocator.dupe(u8, state.plan.decomposed_queries[target_idx]);
        }

        return selected;
    }

    fn applySemanticReranking(self: *Self, results: []types.NewsItem, query: []const u8) ![]types.NewsItem {
        _ = query; // TODO: Use query for semantic reranking

        // Sort by relevance score (simplified reranking)
        const sorted_results = try self.allocator.dupe(types.NewsItem, results);
        std.sort.block(types.NewsItem, sorted_results, {}, struct {
            fn lessThan(_: void, a: types.NewsItem, b: types.NewsItem) bool {
                return a.relevance_score > b.relevance_score;
            }
        }.lessThan);

        return sorted_results;
    }

    fn generateIterationSynthesis(self: *Self, state: *ResearchState, results: []types.NewsItem) ![]const u8 {
        _ = state; // TODO: Use state for more detailed synthesis
        return try std.fmt.allocPrint(self.allocator, "Iteration synthesis: Found {d} relevant sources", .{results.len});
    }

    fn calculateIterationConfidence(self: *Self, results: []types.NewsItem) !f32 {
        _ = self;

        if (results.len == 0) return 0.0;

        var total_score: f32 = 0.0;
        for (results) |result| {
            total_score += result.relevance_score;
        }

        return total_score / @as(f32, @floatFromInt(results.len));
    }

    fn updateResearchPlan(self: *Self, state: *ResearchState, llm_response: []const u8) !void {
        // Parse JSON response for new queries
        const json_start = std.mem.indexOf(u8, llm_response, "{") orelse return;
        const json_end = std.mem.lastIndexOf(u8, llm_response, "}") orelse return;

        if (json_end <= json_start) return;

        const json_slice = llm_response[json_start .. json_end + 1];

        var arena = std.heap.ArenaAllocator.init(self.allocator);
        defer arena.deinit();
        const temp_allocator = arena.allocator();

        const parsed = std.json.parseFromSlice(std.json.Value, temp_allocator, json_slice, .{}) catch return;
        defer parsed.deinit();

        const json_obj = parsed.value.object;

        // Parse new queries
        if (json_obj.get("new_queries")) |queries_val| {
            if (queries_val == .array) {
                // Free old decomposed queries
                for (state.plan.decomposed_queries) |query| {
                    self.allocator.free(query);
                }
                self.allocator.free(state.plan.decomposed_queries);

                // Add new queries
                var new_queries = std.ArrayList([]const u8).init(self.allocator);
                for (queries_val.array.items) |query_val| {
                    if (query_val == .string) {
                        const query_str = try self.allocator.dupe(u8, query_val.string);
                        try new_queries.append(query_str);
                    }
                }

                state.plan.decomposed_queries = try new_queries.toOwnedSlice();
            }
        }

        // Update strategy if provided
        if (json_obj.get("strategy_adjustment")) |strategy_val| {
            if (strategy_val == .string) {
                self.allocator.free(state.plan.search_strategy);
                state.plan.search_strategy = try self.allocator.dupe(u8, strategy_val.string);
            }
        }
    }

    fn parseResearchResult(self: *Self, llm_response: []const u8, state: *ResearchState) !ResearchResult {
        // Try to parse JSON response
        const json_start = std.mem.indexOf(u8, llm_response, "{") orelse {
            return self.createFallbackResult(state);
        };
        const json_end = std.mem.lastIndexOf(u8, llm_response, "}") orelse {
            return self.createFallbackResult(state);
        };

        if (json_end <= json_start) {
            return self.createFallbackResult(state);
        }

        const json_slice = llm_response[json_start .. json_end + 1];

        var arena = std.heap.ArenaAllocator.init(self.allocator);
        defer arena.deinit();
        const temp_allocator = arena.allocator();

        const parsed = std.json.parseFromSlice(std.json.Value, temp_allocator, json_slice, .{}) catch {
            return self.createFallbackResult(state);
        };
        defer parsed.deinit();

        const json_obj = parsed.value.object;

        // Parse comprehensive answer
        const answer = if (json_obj.get("comprehensive_answer")) |answer_val|
            if (answer_val == .string) try self.allocator.dupe(u8, answer_val.string) else try std.fmt.allocPrint(self.allocator, "Research findings for: {s}", .{state.plan.original_query})
        else
            try std.fmt.allocPrint(self.allocator, "Research findings for: {s}", .{state.plan.original_query});

        // Parse key findings
        var findings = std.ArrayList([]const u8).init(self.allocator);
        if (json_obj.get("key_findings")) |findings_val| {
            if (findings_val == .array) {
                for (findings_val.array.items) |finding_val| {
                    if (finding_val == .string) {
                        const finding_str = try self.allocator.dupe(u8, finding_val.string);
                        try findings.append(finding_str);
                    }
                }
            }
        }

        // Fallback findings if none parsed
        if (findings.items.len == 0) {
            try findings.append(try std.fmt.allocPrint(self.allocator, "Found {d} relevant sources for {s}", .{ state.gathered_evidence.items.len, state.plan.original_query }));
            try findings.append(try std.fmt.allocPrint(self.allocator, "Completed {d} search iterations", .{state.current_iteration}));
            try findings.append(try std.fmt.allocPrint(self.allocator, "Applied {s} research strategy", .{state.plan.search_strategy}));
        }

        // Parse confidence
        const confidence = if (json_obj.get("confidence")) |conf_val| blk: {
            switch (conf_val) {
                .float => break :blk @as(f32, @floatCast(conf_val.float)),
                .integer => break :blk @as(f32, @floatFromInt(conf_val.integer)),
                else => break :blk 0.8,
            }
        } else 0.8;

        // Clone evidence sources
        var evidence_copy = try self.allocator.alloc(types.NewsItem, state.gathered_evidence.items.len);
        for (state.gathered_evidence.items, 0..) |evidence, i| {
            evidence_copy[i] = try evidence.clone(self.allocator);
        }

        return ResearchResult{
            .original_query = try self.allocator.dupe(u8, state.plan.original_query),
            .comprehensive_answer = answer,
            .key_findings = try findings.toOwnedSlice(),
            .evidence_sources = evidence_copy,
            .confidence_score = confidence,
            .research_depth = state.current_iteration,
            .total_sources_examined = @as(u32, @intCast(state.gathered_evidence.items.len)),
        };
    }

    fn createFallbackResult(self: *Self, state: *ResearchState) !ResearchResult {
        const answer = try std.fmt.allocPrint(self.allocator, "Based on comprehensive research, here is the summary for: {s}", .{state.plan.original_query});

        var findings = try self.allocator.alloc([]const u8, 3);
        findings[0] = try std.fmt.allocPrint(self.allocator, "Found {d} relevant sources", .{state.gathered_evidence.items.len});
        findings[1] = try std.fmt.allocPrint(self.allocator, "Completed {d} research iterations", .{state.current_iteration});
        findings[2] = try self.allocator.dupe(u8, "Applied multi-source search strategy");

        // Clone evidence sources
        var evidence_copy = try self.allocator.alloc(types.NewsItem, state.gathered_evidence.items.len);
        for (state.gathered_evidence.items, 0..) |evidence, i| {
            evidence_copy[i] = try evidence.clone(self.allocator);
        }

        return ResearchResult{
            .original_query = try self.allocator.dupe(u8, state.plan.original_query),
            .comprehensive_answer = answer,
            .key_findings = findings,
            .evidence_sources = evidence_copy,
            .confidence_score = 0.7,
            .research_depth = state.current_iteration,
            .total_sources_examined = @as(u32, @intCast(state.gathered_evidence.items.len)),
        };
    }

};

/// Simple search interface for LLM-First Search integration
const SimpleSearchInterface = struct {
    deep_research_system: *DeepResearchSystem,

    pub fn search(self: SimpleSearchInterface, query: []const u8) ![]types.NewsItem {
        return try self.deep_research_system.performWebSearch(query);
    }
};

/// Create Deep Research System
pub fn createDeepResearchSystem(allocator: std.mem.Allocator, claude_client: *claude.ClaudeClient, http_client: *http.HttpClient, firecrawl_client: *firecrawl.FirecrawlClient) DeepResearchSystem {
    return DeepResearchSystem.init(allocator, claude_client, http_client, firecrawl_client);
}

// Test function
test "Deep research system initialization" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    var claude_client = claude.ClaudeClient.init(allocator, "sonnet");
    
    var http_client = try http.HttpClient.init(allocator);
    defer http_client.deinit();
    
    var firecrawl_client = try firecrawl.FirecrawlClient.init(allocator, "test-key");
    defer firecrawl_client.deinit();

    var research_system = createDeepResearchSystem(allocator, &claude_client, &http_client, &firecrawl_client);
    defer research_system.deinit();

    try std.testing.expect(research_system.total_research_sessions == 0);
    try std.testing.expect(research_system.default_complexity == .moderate);
}
