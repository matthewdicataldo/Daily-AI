const std = @import("std");
const claude = @import("ai_claude.zig");
const skytable_client = @import("cache_skytable_client.zig");
const unified_skytable = @import("cache_unified_skytable.zig");
const mcp_client = @import("external_mcp_client.zig");
const llm_first_search = @import("ai_llm_first_search.zig");
const deep_research = @import("ai_deep_research.zig");
const json_processor = @import("ai_json_processor.zig");
const web_crawler = @import("extract_web_crawler.zig");
const model_downloader = @import("ai_model_downloader.zig");
const tiktok = @import("extract_tiktok.zig");
const youtube = @import("extract_youtube.zig");
const huggingface = @import("extract_huggingface.zig");
const firecrawl = @import("external_firecrawl.zig");
const gitingest = @import("external_gitingest.zig");
const http = @import("common_http.zig");
const config = @import("core_config.zig");
const types = @import("core_types.zig");

// Optional dependency - comment out if not available
// const llama_backend = @import("llama_direct_backend.zig");

/// Integrated AI research system with all components
pub const IntegratedAISystem = struct {
    allocator: std.mem.Allocator,

    // Core backend services
    claude_client: claude.ClaudeClient,
    skytable_system: unified_skytable.UnifiedSkytableSystem,
    http_client: http.HttpClient,
    firecrawl_client: firecrawl.FirecrawlClient,
    json_processor: json_processor.JSONProcessor,

    // AI and search components
    mcp_client: ?mcp_client.MCPClient,
    search_engine: llm_first_search.LLMFirstSearchEngine,
    research_system: deep_research.DeepResearchSystem,
    web_crawler: web_crawler.WebCrawler,

    // Content extraction clients
    youtube_client: youtube.YouTubeClient,
    tiktok_client: tiktok.TikTokClient,
    huggingface_client: huggingface.HuggingFaceClient,
    gitingest_client: gitingest.GitIngestClient,

    // System state
    initialized: bool,
    model_loaded: bool,

    const Self = @This();

    pub fn init(allocator: std.mem.Allocator) !Self {
        std.log.info("üöÄ Initializing Integrated AI System...", .{});

        // Initialize core backend services
        // TODO: Re-enable when llama_backend is available
        // var llm_backend_instance = try llama_backend.LlamaDirectBackend.init(allocator);
        const skytable_system_instance = try unified_skytable.UnifiedSkytableSystem.initEmbedded(allocator);
        var http_client_instance = try http.HttpClient.init(allocator);

        // Load API keys
        const api_keys = try config.Config.loadApiKeys(allocator);
        defer api_keys.deinit(allocator);

        var firecrawl_client_instance = try firecrawl.FirecrawlClient.init(allocator, api_keys.firecrawl_api_key);
        const json_processor_instance = json_processor.createJSONProcessor(allocator);

        // Initialize Claude client for AI components
        var claude_client_instance = claude.ClaudeClient.init(allocator, "sonnet");

        // Initialize AI and search components with Claude
        const search_engine_instance = llm_first_search.createLLMFirstSearchEngine(allocator, &claude_client_instance);
        var research_system_instance = deep_research.createDeepResearchSystem(allocator, &claude_client_instance, &http_client_instance, &firecrawl_client_instance);
        var web_crawler_instance = web_crawler.createWebCrawler(allocator, &http_client_instance);

        // Initialize content extraction clients
        const youtube_client_instance = youtube.YouTubeClient.init(allocator);
        const tiktok_client_instance = tiktok.TikTokClient.init(allocator);
        const huggingface_client_instance = huggingface.HuggingFaceClient.init(allocator, &firecrawl_client_instance);
        const gitingest_client_instance = gitingest.GitIngestClient.init(allocator, &http_client_instance);

        // Initialize research system components
        research_system_instance.initialize();
        web_crawler_instance.setFirecrawlClient(&firecrawl_client_instance);
        // Note: LLM backend integrations now use Claude directly

        std.log.info("‚úÖ Core system components initialized", .{});

        return Self{
            .allocator = allocator,
            .claude_client = claude_client_instance,
            .skytable_system = skytable_system_instance,
            .http_client = http_client_instance,
            .firecrawl_client = firecrawl_client_instance,
            .json_processor = json_processor_instance,
            .mcp_client = null, // Will be initialized separately if needed
            .search_engine = search_engine_instance,
            .research_system = research_system_instance,
            .web_crawler = web_crawler_instance,
            .youtube_client = youtube_client_instance,
            .tiktok_client = tiktok_client_instance,
            .huggingface_client = huggingface_client_instance,
            .gitingest_client = gitingest_client_instance,
            .initialized = true,
            .model_loaded = false,
        };
    }

    pub fn deinit(self: *Self) void {
        std.log.info("üõë Shutting down Integrated AI System...", .{});

        if (self.mcp_client) |*mcp| {
            mcp.deinit();
        }

        // LLM backend removed - using Claude API instead
        self.skytable_system.deinit();
        self.http_client.deinit();
        self.firecrawl_client.deinit();
        self.json_processor.deinit();
        self.search_engine.deinit();
        self.research_system.deinit();
        self.web_crawler.deinit();

        std.log.info("‚úÖ System shutdown complete", .{});
    }

    /// Download and load the Jan nano model
    pub fn loadJanNanoModel(self: *Self) ![]const u8 {
        if (self.model_loaded) {
            std.log.info("ü§ñ Model already loaded", .{});
            return "Claude API (external)";
        }

        std.log.info("üì¶ Downloading Jan nano model...", .{});

        const model_path = try model_downloader.downloadJanNanoModel(self.allocator, &self.http_client, "models");

        std.log.info("üîÑ Loading model into LLM backend...", .{});
        // Using Claude API instead of local LLM

        self.model_loaded = true;
        std.log.info("‚úÖ Jan nano model loaded successfully", .{});

        return model_path;
    }

    /// Initialize MCP client connection
    pub fn initializeMCP(self: *Self, server_url: []const u8) !void {
        std.log.info("üîå Initializing MCP client connection...", .{});

        var mcp_client_instance = try mcp_client.createMCPClient(self.allocator, &self.http_client, server_url);
        try mcp_client_instance.connect();

        // Set LLM backend for AI-powered operations
        // Using Claude API instead of local LLM backend

        // Integrate with research system
        self.research_system.setMCPClient(&mcp_client_instance);

        self.mcp_client = mcp_client_instance;

        std.log.info("‚úÖ MCP client initialized and connected", .{});
    }

    /// Execute comprehensive deep research
    pub fn executeDeepResearch(self: *Self, query: []const u8, complexity: ?deep_research.ResearchComplexity) !deep_research.ResearchResult {
        if (!self.initialized) {
            return error.SystemNotInitialized;
        }

        if (!self.model_loaded) {
            _ = try self.loadJanNanoModel();
        }

        std.log.info("üî¨ Starting deep research: {s}", .{query});

        return try self.research_system.executeDeepResearch(query, complexity);
    }

    /// Extract content from all configured sources
    pub fn extractAllContent(self: *Self) !types.ContentSummary {
        if (!self.initialized) {
            return error.SystemNotInitialized;
        }

        std.log.info("üì• Extracting content from all sources...", .{});

        var all_content = std.ArrayList(types.NewsItem).init(self.allocator);
        defer all_content.deinit();

        // Extract YouTube content
        const youtube_content = try youtube.extractAllYouTubeVideos(self.allocator, {});
        defer {
            for (youtube_content) |item| {
                item.deinit(self.allocator);
            }
            self.allocator.free(youtube_content);
        }

        for (youtube_content) |item| {
            try all_content.append(try item.clone(self.allocator));
        }

        // Extract TikTok content
        const tiktok_content = try tiktok.extractAllTikTokVideos(self.allocator, {});
        defer {
            for (tiktok_content) |item| {
                item.deinit(self.allocator);
            }
            self.allocator.free(tiktok_content);
        }

        for (tiktok_content) |item| {
            try all_content.append(try item.clone(self.allocator));
        }

        // Extract HuggingFace research papers
        const research_content = try huggingface.extractAllResearchPapers(self.allocator, &self.firecrawl_client);
        defer {
            for (research_content) |item| {
                item.deinit(self.allocator);
            }
            self.allocator.free(research_content);
        }

        for (research_content) |item| {
            try all_content.append(try item.clone(self.allocator));
        }

        // Sort by relevance
        const final_content = try all_content.toOwnedSlice();
        std.sort.block(types.NewsItem, final_content, {}, struct {
            fn lessThan(_: void, a: types.NewsItem, b: types.NewsItem) bool {
                return a.relevance_score > b.relevance_score;
            }
        }.lessThan);

        std.log.info("‚úÖ Content extraction completed: {d} items", .{final_content.len});

        return types.ContentSummary{
            .items = final_content,
            .total_count = @as(u32, @intCast(final_content.len)),
            .youtube_count = @as(u32, @intCast(youtube_content.len)),
            .tiktok_count = @as(u32, @intCast(tiktok_content.len)),
            .research_count = @as(u32, @intCast(research_content.len)),
        };
    }

    /// Perform AI-guided web search
    pub fn performAISearch(self: *Self, query: []const u8) ![]types.NewsItem {
        if (!self.initialized) {
            return error.SystemNotInitialized;
        }

        if (!self.model_loaded) {
            _ = try self.loadJanNanoModel();
        }

        std.log.info("üîç Performing AI-guided search: {s}", .{query});

        return try self.search_engine.search(query, &self.research_system);
    }

    /// Crawl and extract content from a URL
    pub fn crawlURL(self: *Self, url: []const u8) !web_crawler.CrawlResult {
        if (!self.initialized) {
            return error.SystemNotInitialized;
        }

        return try self.web_crawler.crawlURL(url);
    }

    /// Process JSON data with high performance
    pub fn processJSON(self: *Self, json_data: []const u8) !json_processor.FastJSONValue {
        if (!self.initialized) {
            return error.SystemNotInitialized;
        }

        return try self.json_processor.parseJSON(json_data);
    }

    /// Benchmark system performance
    pub fn benchmarkPerformance(self: *Self) !SystemBenchmark {
        if (!self.initialized) {
            return error.SystemNotInitialized;
        }

        std.log.info("üèÉ Running system performance benchmark...", .{});

        const start_time = std.time.milliTimestamp();

        // Benchmark JSON processing
        const test_json = "{\"test\": true, \"number\": 42, \"array\": [1, 2, 3]}";
        const json_benchmark = try self.json_processor.benchmarkPerformance(test_json, 1000);

        // Benchmark cache operations
        const cache_benchmark = try self.skytable_system.benchmarkPerformance(100);

        // LLM benchmarking removed - using Claude API instead
        const llm_benchmark: ?LLMBenchmark = null;

        const end_time = std.time.milliTimestamp();
        const total_time = @as(u64, @intCast(end_time - start_time));

        std.log.info("‚úÖ Benchmark completed in {d}ms", .{total_time});

        return SystemBenchmark{
            .total_time_ms = total_time,
            .json_processing = json_benchmark,
            .cache_performance = cache_benchmark,
            .llm_performance = llm_benchmark,
        };
    }

    /// Get system status and health
    pub fn getSystemStatus(self: *Self) SystemStatus {
        return SystemStatus{
            .initialized = self.initialized,
            .model_loaded = self.model_loaded,
            .mcp_connected = self.mcp_client != null,
            .cache_health = self.skytable_system.getMetrics(),
            .components_active = .{
                .llm_backend = false, // Using Claude API instead
                .search_engine = true,
                .research_system = true,
                .web_crawler = true,
                .json_processor = true,
            },
        };
    }
};

/// System benchmark results
pub const SystemBenchmark = struct {
    total_time_ms: u64,
    json_processing: json_processor.BenchmarkResult,
    cache_performance: unified_skytable.PerformanceReport,
    llm_performance: ?LLMBenchmark,
};

/// LLM performance metrics
pub const LLMBenchmark = struct {
    generation_time_ms: u64,
    tokens_per_second: f64,
};

/// System status information
pub const SystemStatus = struct {
    initialized: bool,
    model_loaded: bool,
    mcp_connected: bool,
    cache_health: unified_skytable.SkytableMetrics,
    components_active: ComponentStatus,
};

/// Component status flags
pub const ComponentStatus = struct {
    llm_backend: bool,
    search_engine: bool,
    research_system: bool,
    web_crawler: bool,
    json_processor: bool,
};

/// Create and initialize the integrated AI system
pub fn createIntegratedAISystem(allocator: std.mem.Allocator) !IntegratedAISystem {
    return try IntegratedAISystem.init(allocator);
}

// Test function
test "Integrated AI system initialization" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // This test would require actual system resources
    // For now, just test the creation doesn't crash
    var system = createIntegratedAISystem(allocator) catch |err| {
        // Expected to fail in test environment without proper setup
        if (err == error.ConnectionFailed or err == error.ModelLoadFailed) {
            return;
        }
        return err;
    };
    defer system.deinit();

    const status = system.getSystemStatus();
    try std.testing.expect(status.initialized);
}
